# Transfomer
## necessary libraries

```bash
!pip install transformers datasets evaluate accelerate torch
```

## pipeline
`pipeline()`å‡½æ•°ï¼šä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹å’Œtokenizerè¿›è¡Œæ¨ç†ï¼Œ[æ”¯æŒå¤šç§task](https://huggingface.co/docs/transformers/quicktour#pipeline)

```python
from transformers import pipeline
classifier = pipeline("sentiment-analysis")

classifier("We are very happy to show you the ğŸ¤— Transformers library.") # single query
[{'label': 'POSITIVE', 'score': 0.9998}]

results = classifier(["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."]) # multiple queries
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
label: POSITIVE, with score: 0.9998
label: NEGATIVE, with score: 0.5309
```

### æŒ‡å®šæ¨¡å‹å’ŒåŠŸèƒ½
#### é€šè¿‡æ¨¡å‹åæŒ‡å®šæ¨¡å‹
```python
import torch
from transformers import pipeline
from datasets import load_dataset, Audio

speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")
dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")
dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate)) # match sampling rate, can ignore
result = speech_recognizer(dataset[:4]["audio"])
print([d["text"] for d in result])
```

#### é€šè¿‡ `AutoClass` æŒ‡å®šæ¨¡å‹
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"

# download and cache pretrained model and tokenizer
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ğŸ¤— Transformers.")
```

## AutoClass
### AutoTokenizer
ç”¨æ³•å¦‚ä¸Šï¼Œtokenizerè¿”å›ä¸€ä¸ª`dict`å¯¹è±¡ï¼š

+ input_ids: numerical representations of your tokens.
+ attention_mask: indicates which tokens should be attended to.

```python
encoding = tokenizer("We are very happy to show you the ğŸ¤— Transformers library.")
print(encoding)

{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}

# tokenize in batch
pt_batch = tokenizer(
    ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt", # return pytorch tensor, not tensorflow tensor
)
```

### AutoModel
é»˜è®¤æƒ…å†µä¼šå°†æƒé‡è®¾ç½®ä¸ºfull precision (torch.float32)ï¼Œè®¾ç½® `torch_dtype="auto"` å¯ä»¥å°†æ•°æ®ç±»å‹è®¾ç½®ä¸ºthe most memory-optimal data type

```python
from transformers import AutoModelForSequenceClassification
from torch import nn

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name, torch_dtype="auto")

# feed the tokenized dict (add ** to unpack the dict) to the model
# The model outputs the final activations in the logits attribute
pt_outputs = pt_model(**pt_batch)

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)
```

> All ğŸ¤— Transformers models (PyTorch or TensorFlow) output the tensors before the final activation function (like softmax) because the final activation function is often fused with the loss.

### save a model
```python
pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)

# reload local model
pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory)
```

ä¹Ÿå¯ä»¥é€šè¿‡`from_pt`å’Œ`from_tf`å°†å­˜å‚¨çš„æ¨¡å‹è½¬æ¢ä¸º pytorch æˆ– tensorflow æ¶æ„

```python
from transformers import AutoModel

tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)
```

## Customized configuration
å¯ä»¥æ”¹å˜hidden layer å±‚æ•°ã€attention head æ•°ç­‰ï¼Œä½†éœ€è¦æ³¨æ„æ­¤æ—¶æ¨¡å‹å‚æ•°æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œéœ€è¦é‡æ–°è®­ç»ƒ

```python
from transformers import AutoConfig, AutoModel

my_config = AutoConfig.from_pretrained("distilbert/distilbert-base-uncased", n_heads=12)
my_model = AutoModel.from_config(my_config)
```

## Trainer - a PyTorch optimized training loop
`torch.nn.Module` çš„è®­ç»ƒè¿‡ç¨‹å¯ä»¥è‡ªå®šä¹‰ï¼Œä¹Ÿå¯ä»¥ç”¨å°è£…å¥½çš„ `Trainer` ç±»ï¼Œå®ƒæä¾›äº†åŸºæœ¬çš„training loop ï¼Œæ”¯æŒ distributed training, mixed precision, and more.

```python
from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased", torch_dtype="auto")

from transformers import TrainingArguments
training_args = TrainingArguments(
    output_dir="path/to/save/folder/",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=2,
)

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")

from datasets import load_dataset
dataset = load_dataset("rotten_tomatoes")  # doctest: +IGNORE_RESULT

# Create a function to tokenize the dataset
def tokenize_dataset(dataset):
    return tokenizer(dataset["text"])
# apply it over the entire dataset with map
dataset = dataset.map(tokenize_dataset, batched=True)
# A DataCollatorWithPadding to create a batch of examples from your dataset:
from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# gather all these classes in Trainer:
from transformers import Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    processing_class=tokenizer,
    data_collator=data_collator,
)  # doctest: +SKIP

# call train() to start training
trainer.train()
```
